import google.generativeai as genai
import logging
import json
from typing import List, Dict, Generator, AsyncGenerator
import os
from pathlib import Path
import asyncio
from dataclasses import dataclass
from enum import Enum
import time

class ProcessingStage(Enum):
    UPLOADING = "uploading"
    ANALYZING = "analyzing"
    EXTRACTING_METADATA = "extracting_metadata"
    PROCESSING_CHAPTERS = "processing_chapters"
    ENHANCING_CONTENT = "enhancing_content"
    COMPLETE = "complete"
    ERROR = "error"

@dataclass
class ProcessingProgress:
    stage: ProcessingStage
    progress: float  # 0-100
    message: str
    details: Dict = None

class StreamingDocumentProcessor:
    def __init__(self):
        genai.configure(api_key=os.getenv('GEMINI_API_KEY'))
        self.model = genai.GenerativeModel('gemini-1.5-flash-8b')
        self.logger = logging.getLogger(__name__)
        
    async def process_document_streaming(
        self, file_path: str
    ) -> AsyncGenerator[ProcessingProgress, None]:
        """
        Process document with progress updates and streaming results
        Returns an async generator of ProcessingProgress objects
        """
        try:
            # Upload file
            yield ProcessingProgress(
                stage=ProcessingStage.UPLOADING,
                progress=0,
                message="Starting file upload"
            )
            
            uploaded_file = genai.upload_file(Path(file_path))
            
            yield ProcessingProgress(
                stage=ProcessingStage.UPLOADING,
                progress=100,
                message="File upload complete",
                details={"file_name": uploaded_file.name}
            )

            # Initial metadata extraction
            yield ProcessingProgress(
                stage=ProcessingStage.ANALYZING,
                progress=0,
                message="Analyzing document structure"
            )

            # First pass - get document structure and metadata
            structure_prompt = """Analyze this document and return only the basic structure:
            1. Total number of chapters
            2. Estimated paragraphs per chapter
            3. Basic metadata (title, author, year, genre)
            
            Return as JSON."""

            structure_response = await self.model.generate_content(
                [structure_prompt, uploaded_file])
            structure_data = json.loads(structure_response.text)

            yield ProcessingProgress(
                stage=ProcessingStage.EXTRACTING_METADATA,
                progress=30,
                message="Retrieved document structure",
                details=structure_data
            )

            # Process chapters in batches
            total_chapters = structure_data.get('total_chapters', 1)
            chapters_data = []

            for chapter_num in range(total_chapters):
                progress = (chapter_num / total_chapters) * 100
                
                yield ProcessingProgress(
                    stage=ProcessingStage.PROCESSING_CHAPTERS,
                    progress=progress,
                    message=f"Processing chapter {chapter_num + 1}/{total_chapters}",
                    details={"current_chapter": chapter_num + 1}
                )

                chapter_prompt = f"""Process chapter {chapter_num + 1} of this document.
                Return a JSON object with:
                1. Chapter title
                2. Array of paragraphs with their text
                3. Summary of the chapter
                
                Return only the JSON object."""

                chapter_response = await self.model.generate_content(
                    [chapter_prompt, uploaded_file])
                chapter_data = json.loads(chapter_response.text)
                chapters_data.append(chapter_data)

            yield ProcessingProgress(
                stage=ProcessingStage.ENHANCING_CONTENT,
                progress=0,
                message="Enhancing content with media guidance"
            )

            # Enhance paragraphs with media guidance
            enhanced_chapters = []
            total_paragraphs = sum(len(ch.get('paragraphs', [])) 
                                 for ch in chapters_data)
            processed_paragraphs = 0

            for chapter in chapters_data:
                enhanced_chapter = chapter.copy()
                enhanced_paragraphs = []
                
                # Process paragraphs in batches
                batch_size = 5
                paragraphs = chapter.get('paragraphs', [])
                
                for i in range(0, len(paragraphs), batch_size):
                    batch = paragraphs[i:i + batch_size]
                    
                    # Update progress
                    processed_paragraphs += len(batch)
                    progress = (processed_paragraphs / total_paragraphs) * 100
                    
                    yield ProcessingProgress(
                        stage=ProcessingStage.ENHANCING_CONTENT,
                        progress=progress,
                        message=f"Enhancing content ({processed_paragraphs}/{total_paragraphs} paragraphs)",
                        details={
                            "processed_paragraphs": processed_paragraphs,
                            "total_paragraphs": total_paragraphs
                        }
                    )

                    # Generate media guidance for batch
                    enhancement_prompt = """For each paragraph, provide:
                    1. A detailed image generation prompt
                    2. Audio narration guidance
                    
                    Return array of JSON objects with keys: 
                    "image_prompt", "narration_guidance"
                    
                    Paragraphs:
                    """
                    
                    enhancement_prompt += "\n".join(
                        f"{i+1}. {p['text']}" for i, p in enumerate(batch)
                    )

                    enhancement_response = await self.model.generate_content(
                        enhancement_prompt)
                    enhancements = json.loads(enhancement_response.text)

                    # Combine original paragraphs with enhancements
                    for p, e in zip(batch, enhancements):
                        enhanced_paragraph = {
                            'text': p['text'],
                            'image_prompt': e['image_prompt'],
                            'narration_guidance': e['narration_guidance']
                        }
                        enhanced_paragraphs.append(enhanced_paragraph)

                enhanced_chapter['paragraphs'] = enhanced_paragraphs
                enhanced_chapters.append(enhanced_chapter)

            # Compile final result
            final_result = {
                'metadata': structure_data['metadata'],
                'chapters': enhanced_chapters
            }

            # Clean up
            uploaded_file.delete()

            yield ProcessingProgress(
                stage=ProcessingStage.COMPLETE,
                progress=100,
                message="Document processing complete",
                details=final_result
            )

        except Exception as e:
            self.logger.error(f"Error processing document: {str(e)}")
            yield ProcessingProgress(
                stage=ProcessingStage.ERROR,
                progress=0,
                message=f"Error: {str(e)}"
            )
            raise

    async def process_large_document(
        self, file_path: str, chunk_size: int = 20
    ) -> AsyncGenerator[ProcessingProgress, None]:
        """
        Process very large documents by splitting into manageable chunks
        """
        try:
            # Get document size
            file_size = os.path.getsize(file_path)
            total_chunks = (file_size + chunk_size - 1) // chunk_size
            
            for chunk_num in range(total_chunks):
                progress = (chunk_num / total_chunks) * 100
                
                yield ProcessingProgress(
                    stage=ProcessingStage.PROCESSING_CHAPTERS,
                    progress=progress,
                    message=f"Processing chunk {chunk_num + 1}/{total_chunks}",
                    details={"current_chunk": chunk_num + 1}
                )

                # Process chunk using the regular streaming processor
                async for progress in self.process_document_streaming(file_path):
                    # Modify progress to reflect overall chunk progress
                    chunk_progress = (chunk_num + (progress.progress / 100)) / total_chunks * 100
                    yield ProcessingProgress(
                        stage=progress.stage,
                        progress=chunk_progress,
                        message=progress.message,
                        details=progress.details
                    )

        except Exception as e:
            self.logger.error(f"Error processing large document: {str(e)}")
            yield ProcessingProgress(
                stage=ProcessingStage.ERROR,
                progress=0,
                message=f"Error: {str(e)}"
            )
            raise